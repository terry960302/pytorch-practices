{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화 리뷰 감정 분석\n",
    "1. 다대일 RNN 방식\n",
    "2. 후반부에 softmax로 긍정의 감정인지 부정의 감정인지 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리뷰를 토큰으로 나누기\n",
    "토큰 : 언어의 최소 단위(한글에서는 형태소)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets # torchtext==0.4버전을 다운받아야 에러가 안먹음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 정의\n",
    "하이퍼파라미터 : 인공지능 학습 모델에 있어 성능에 영향을 미치는 인간이 정해주는 변수(선험적 지식에 의해, 감에 의해 정해지는 경우가 다수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 40\n",
    "USE_CUDA = torch.cuda.is_available() # gpu 사용설정\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 제작\n",
    "1. 훈련 데이터\n",
    "2. 테스트 데이터\n",
    "3. 검증 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, batch_first=True, lower = True) # 순차적인 데이터셋(RNN 적용할 것이기 때문), 영어 소문자로 표시\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분리\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL) \n",
    "\n",
    "TEXT.build_vocab(trainset, min_freq=5) # 만들어진 데이터셋으로 워드 임베딩에 필요한 word 사전을 제작(최소 5번 이상 등장한 단어만 사전에 등재, 5번 미만은 unk로 표시됨.)\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터\n",
    "\n",
    "trainset, valset = trainset.split(split_ratio=0.8) # 훈련 : 검증 = 8 : 2\n",
    "\n",
    "# 배치단위로 훈련, 검증, 테스트 데이터를 생성해주는 반복 iterator 생성\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    datasets=(trainset, valset, testset),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 속 단어들의 개수와 레이블의 수를 정해주는 변수\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습셋] : 20000, [검증셋] : 5000, [테스트셋] : 25000, [단어수] : 46159, [클래스] : 2\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 확인\n",
    "print(\"[학습셋] : {}, [검증셋] : {}, [테스트셋] : {}, [단어수] : {}, [클래스] : {}\".format(len(trainset), len(valset), len(testset), vocab_size, n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다대일 RNN 모델 구현\n",
    "\n",
    "왜 RNN이 아니라 GRU인가?\n",
    "=> 시계열 데이터를 학습하다보면 데이터가 길 경우 앞에 있던 맥락을 잊기 쉽다. 그리고 이 과정에서 기울기가 너무 작아지거나 너무 커지는 \"기울기 폭발/소실\" 현상이 발생한다. 이를 방지하고자 \"게이트\"라는 개념을 도입한 GRU를 통해 앞 내용을 얼마나 기억할지 조합할지를 구분한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(BasicGRU, self).__init__()\n",
    "        \n",
    "        print(\"Building Basic GRU model...\")\n",
    "        \n",
    "        self.n_layers = n_layers # 층 \n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim) # 임베딩된 단어 텐서가 지니는 차원값\n",
    "        self.hidden_dim = hidden_dim # 은닉 벡터의 차원값\n",
    "        self.dropout = nn.Dropout(dropout_p) # 정규화를 위한 드롭아웃\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True) # 경사도 폭발 및 소실을 막기 위해 GRU를 사용\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes) # 긍정인지 부정인지 클래스 분류\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # x = 하나의 배치 속에 들어있는 영화 리뷰들, embed 함수를 통하여 문장을 벡터 배열로 변환\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # h_0은 은닉벡터로 여기서 처음 정의함.\n",
    "        x, _ = self.gru(x, h_0) # 영화 리뷰 벡터들과 은닉벡터들을 시계열 벡터 형태로 변환해서 재할당\n",
    "        h_t = x[:,-1,:] # 시계열 벡터 중 은닉벡터 부분만 할당 (이게 곧 영화 리뷰 배열들을 압축한 은닉벡터)\n",
    "        self.dropout(h_t) # 드롭아웃 설정\n",
    "        logit = self.out(h_t) # 신경망에 압축한 은닉벡터를 입력하여 결과 출력\n",
    "        return logit\n",
    "    \n",
    "    def _init_state(self, batch_size=1): # 초기 상태를 제작\n",
    "        weight = next(self.parameters()).data # GRU 모듈에서 첫번째 가중치 텐서를 추출\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() # 모델의 가중치 모양과 맞게 조정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습(모델을 최적화해나가는 과정, 답을 맞춰나가는 모델을 만드는 과정)\n",
    "def train(model, optimizer, train_iter):\n",
    "        model.train()\n",
    "        \n",
    "        for b, batch in enumerate(train_iter): # 매 반복마다 훈련 데이터 배치 하나를 갖고 처리\n",
    "            \n",
    "            x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "            y.data.sub_(1) # label값을 0과 1로 변환(핫인코딩)\n",
    "            optimizer.zero_grad() # 기울기 초기화\n",
    "            \n",
    "            logit = model(x) # 학습 데이터를 입력하여 예측값을 반환\n",
    "            loss = F.cross_entropy(logit, y) # 예측값과 실제값의 오차를 반환\n",
    "            loss.backward() # 오차로 역전파를 시행\n",
    "            optimizer.step() # 오차 업데이트에서 optim에 반영\n",
    "\n",
    "# 모델 평가(검증 데이터와 비교했을 때 훈련에서의 예측이 잘 맞았는지 검증)\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0 # 맞은 개수와 오차의 총합\n",
    "\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data._sub_(1)\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, reduction=\"sum\") # 오차의 합을 구함.\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum() \n",
    "\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Basic GRU model...\n"
     ]
    }
   ],
   "source": [
    "model = BasicGRU(n_layers=2, hidden_dim=256, n_vocab=vocab_size, embed_dim=128, n_classes=n_classes, dropout_p=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1960662137d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-202b1478a561>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_iter)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = None # 검증오차가 가장 적은 모델이 가장 좋은 모델이므로 검증오차가 제일 적을 때 모델을 저장하자\n",
    "\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    \n",
    "    train(model, optimizer, train_iter)\n",
    "    \n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "    \n",
    "    print(\"[{}회차] 검증 오차 : {:5.2f} | 검증 정확도 : {:5.2f}\".format(e, val_loss, val_accuracy))\n",
    "    \n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss  < best_val_loss:\n",
    "        \n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        \n",
    "        torch.save(model.state_dict(), \"./snapshot/txtclassification.pt\")\n",
    "        \n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./snapshot/txtclassification.pt\"))\n",
    "test_loss, test_acc = evaluate(model, test_iter)\n",
    "\n",
    "print(\"테스트 오차 : {:5.2f} | 테스트 정확도 : {:5.2f}\".format(test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
